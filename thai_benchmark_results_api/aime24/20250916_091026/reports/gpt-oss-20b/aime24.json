{
    "name": "gpt-oss-20b@aime24",
    "dataset_name": "aime24",
    "dataset_pretty_name": "AIME-2024",
    "dataset_description": "The AIME 2024 benchmark is based on problems from the American Invitational Mathematics Examination, a prestigious high school mathematics competition. This benchmark tests a model's ability to solve challenging mathematics problems by generating step-by-step solutions and providing the correct final answer.",
    "model_name": "gpt-oss-20b",
    "score": 1.0,
    "metrics": [
        {
            "name": "mean_acc",
            "num": 1,
            "score": 1.0,
            "macro_score": 1.0,
            "categories": [
                {
                    "name": [
                        "default"
                    ],
                    "num": 1,
                    "score": 1.0,
                    "macro_score": 1.0,
                    "subsets": [
                        {
                            "name": "default",
                            "score": 1.0,
                            "num": 1
                        }
                    ]
                }
            ]
        }
    ],
    "analysis": "N/A"
}