analysis_report: false
api_url: http://localhost:8801/v1/chat/completions
chat_template: null
dataset_args:
  humaneval-th:
    aggregation: mean
    dataset_id: iapp/openai_humaneval-th
    default_subset: default
    description: HumanEval Thai is a benchmark for evaluating the ability of code
      generation models to write Python functions based on given specifications in
      Thai language.
    eval_split: test
    extra_params:
      dataset_hub: huggingface
      num_workers: 4
      timeout: 4
    few_shot_num: 0
    few_shot_prompt_template: null
    few_shot_random: false
    filters: null
    metric_list:
    - Pass@1
    name: humaneval-th
    output_types:
    - generation
    pretty_name: HumanEval-Thai
    prompt_template: 'คุณคือผู้เชี่ยวชาญด้านการเขียนโค้ด จงคิดวิเคราะห์คำถามแต่ละข้อและแสดงกระบวนการคิด
      เริ่มกระบวนการคิดด้วย <think> และจบด้วย </think>


      เขียนโค้ดให้สมบูรณ์ตามที่กำหนด:

      {question}


      เขียนเฉพาะโค้ดคำตอบสุดท้ายระหว่าง ``` และ ```'
    query_template: null
    shuffle: false
    shuffle_choices: false
    subset_list:
    - default
    system_prompt: null
    tags:
    - Coding
    train_split: null
dataset_dir: /home/saiuser/.cache/modelscope/hub/datasets
dataset_hub: huggingface
datasets:
- humaneval-th
debug: false
eval_backend: Native
eval_batch_size: 1
eval_config: null
eval_type: openai_api
generation_config:
  batch_size: 1
  do_sample: false
  max_new_tokens: 16384
  temperature: 0.0
  timeout: 300.0
ignore_errors: false
judge_model_args: {}
judge_strategy: auto
judge_worker_num: 1
limit: 10
model: gpt-oss-20b
model_args: {}
model_id: gpt-oss-20b
model_task: text_generation
repeats: 1
rerun_review: false
seed: 42
stream: null
timeout: 300.0
use_cache: null
work_dir: test_output_api_humaneval-th/20250916_035950
